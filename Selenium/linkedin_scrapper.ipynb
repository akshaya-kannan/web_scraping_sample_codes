{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta, datetime\n",
    "from IPython.core.display import clear_output\n",
    "from requests import get\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep\n",
    "from time import time\n",
    "from warnings import warn\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import selenium.webdriver.support.ui as ui\n",
    "\n",
    "#copy paste the below given link and do the filter you want and paste the url again here\n",
    "driver = webdriver.Chrome('C:/Users/<your-path>/chromedriver.exe')\n",
    "url = \"https://www.linkedin.com/jobs/search?keywords=Data%2BEngineer&location=United%2BStates&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&f_TPR=r2592000&position=1&pageNum=0\"\n",
    "keyword = url.split(\"keywords=\")[1].split(\"&location\")[0].replace(\"%2B\",\"-\")\n",
    "timestr = time.strftime(\"%Y%m%d\")\n",
    "driver.get(url)\n",
    "sleep(3)\n",
    "action = ActionChains(driver)\n",
    "\n",
    "\n",
    "#you can change the iteration according to your needs eg: (0,25) or (0,100)\n",
    "#This code is to scroll and fetch the data\n",
    "for i in range(0,50):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    try:\n",
    "        WebDriverWait(driver, 4).until(EC.element_to_be_clickable((By.XPATH, \"//button[@aria-label = 'Load more results']\"))).click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#extract the information \n",
    "lists=WebDriverWait(driver, 10).until(lambda x: x.find_elements_by_xpath('//ul[@class=\"jobs-search__results-list\"]/li'))\n",
    "all_data = []\n",
    "for li in lists:\n",
    "    try:\n",
    "        job_title = WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\".//h3[@class='base-search-card__title']\")).text\n",
    "    except:\n",
    "        job_title = None\n",
    "    try:\n",
    "        company_name = WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\".//h4[@class='base-search-card__subtitle']\")).text\n",
    "    except:\n",
    "        company_name = None\n",
    "    try:\n",
    "        job_location = WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\".//span[@class='job-search-card__location']\")).text\n",
    "    except:\n",
    "        job_location = None\n",
    "    try:\n",
    "        job_id = WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\"./div[1]\")).get_attribute(\"data-entity-urn\")\n",
    "        job_id = job_id.split(\"jobPosting:\")[1]\n",
    "    except:\n",
    "        job_id = None\n",
    "    try:\n",
    "        job_url = WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\".//a[@class='base-card__full-link']\")).get_attribute(\"href\")\n",
    "    except:\n",
    "        job_url = None\n",
    "    try:\n",
    "        job_posted_dt=WebDriverWait(li, 10).until(lambda x: x.find_element_by_xpath(\".//time[@class='job-search-card__listdate']\")).get_attribute(\"datetime\")\n",
    "    except:\n",
    "        job_posted_dt = None\n",
    "    result_dict = {\"job_title\":job_title,\n",
    "                  \"company_name\": company_name,\n",
    "                  \"job_location\": job_location,\n",
    "                  \"job_id\": job_id,\n",
    "                  \"job_url\": job_url,\n",
    "                  \"job_posted_dt\":job_posted_dt}\n",
    "    df_1 = pd.DataFrame([result_dict])\n",
    "    df_1.to_csv('{}_urls_{}.csv'.format(keyword,timestr), mode='a', header=False)\n",
    "    all_data.append(result_dict)\n",
    "#         time.sleep(2)\n",
    "\n",
    "#To Extract further information by navigating to the respective urls\n",
    "df = pd.DataFrame(all_data)\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df.to_csv(\"{}_intermediate_{}.csv\".format(keyword,timestr))\n",
    "print(\"URLs extracted from main page...\")\n",
    "seniority_level = []\n",
    "employment_type = []\n",
    "job_function = []\n",
    "industry = []\n",
    "for url in df[\"job_url\"]:\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        sleep(1)\n",
    "        action = ActionChains(driver)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        seniority_1 = WebDriverWait(driver, 5).until(lambda x: x.find_element_by_xpath(\"//*[@id='main-content']/section[1]/section[3]/ul//h3[contains(text(),'Seniority level')]/following-sibling::span\")).text\n",
    "    except:\n",
    "        seniority_1 = None\n",
    "    seniority_level.append(seniority_1)\n",
    "    try:\n",
    "        employment_1 = WebDriverWait(driver, 5).until(lambda x: x.find_element_by_xpath(\"//*[@id='main-content']/section[1]/section[3]/ul//h3[contains(text(),'Employment type')]/following-sibling::span\")).text\n",
    "    except:\n",
    "        employment_1 = None\n",
    "    employment_type.append(employment_1)\n",
    "    try:\n",
    "        job_function_1 = WebDriverWait(driver, 5).until(lambda x: x.find_element_by_xpath(\"//*[@id='main-content']/section[1]/section[3]/ul//h3[contains(text(),'Job function')]/following-sibling::span\")).text\n",
    "    except:\n",
    "        job_function_1 = None\n",
    "    job_function.append(job_function_1)\n",
    "    try:\n",
    "        industry_1 = WebDriverWait(driver, 5).until(lambda x: x.find_element_by_xpath(\"//*[@id='main-content']/section[1]/section[3]/ul//h3[contains(text(),'Industries')]/following-sibling::span\")).text\n",
    "    except:\n",
    "        industry_1 = None\n",
    "    industry.append(industry_1)\n",
    "df[\"seniority_level\"] = seniority_level\n",
    "df[\"employment_type\"] = employment_type\n",
    "df[\"job_function\"] = job_function\n",
    "df[\"industry\"] = industry\n",
    "df[\"job_flag\"] = keyword\n",
    "print(\"Extracted the information completely\")\n",
    "df.to_csv(\"{}_final_{}.csv\".format(keyword,timestr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
